{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#installations"
      ],
      "metadata": {
        "id": "PmP0YoFG8-iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "rDK2RdC-6ZWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#imports"
      ],
      "metadata": {
        "id": "zLNQXu9x9Ae3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from colorama import Fore\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "JRW60Q7c9D1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing + Model Creation + Training Phase"
      ],
      "metadata": {
        "id": "A5tmFb-5-Cvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "RHfXlV-a4g0P"
      },
      "outputs": [],
      "source": [
        "def summarize_text_word2vec(text:str, num_sentences:int=3) -> str:\n",
        "\n",
        "  sentences:list = nltk.sent_tokenize(text)\n",
        "  words = [word_tokenize(sentence.lower()) for sentence in sentences] # Sentence Tokenization.\n",
        "\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [[word for word in sentence if word.isalnum() and word not in stop_words] for sentence in words] # Stop Words Removal.\n",
        "\n",
        "  # Train a Word2Vec model on the text.\n",
        "  model = Word2Vec(words, vector_size=300, window=5, min_count=1, workers=4)  # Increased vector size\n",
        "\n",
        "\n",
        "  sentence_embeddings:list = []  # Calculating sentence embeddings.\n",
        "  for sentence in words:\n",
        "    sentence_embedding = np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
        "    if sentence_embedding is not None:\n",
        "      sentence_embeddings.append(sentence_embedding)\n",
        "\n",
        "  # Calculate sentence similarity using cosine similarity.\n",
        "  similarity_matrix = cosine_similarity(sentence_embeddings)\n",
        "\n",
        "  # Select the most important sentences based on similarity scores.\n",
        "  sentence_scores = np.sum(similarity_matrix, axis=1)\n",
        "  ranked_sentences = sorted(((score, index) for index, score in enumerate(sentence_scores)), reverse=True)\n",
        "  summary_sentences = [sentences[index] for score, index in ranked_sentences[:num_sentences]]\n",
        "\n",
        "  # Return the summary.\n",
        "  return \" \".join(summary_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text:str = \"\"\"Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words.\n",
        " These vectors capture information about the meaning of the word based on the surrounding words.\n",
        " The word2vec algorithm estimates these representations by modeling text in a large corpus.\n",
        " Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.\n",
        " Word2vec was developed by Tom치코 Mikolov and colleagues at Google and published in 2013.\n",
        "Word2vec represents a word as a high-dimension vector of numbers which capture relationships between words.\n",
        "In particular, words which appear in similar contexts are mapped to vectors which are nearby as measured by cosine similarity. This indicates the level of semantic similarity between the words,\n",
        " so for example the vectors for walk and ran are nearby, as are those for \"but\" and \"however\", and \"Berlin\" and \"Germany\".\"\"\"\n",
        "\n",
        "summary:str = summarize_text_word2vec(text)"
      ],
      "metadata": {
        "id": "17j2K3t69u6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "{Fore.BLUE }\n",
        "Original:\n",
        "\n",
        "{text}\n",
        "\n",
        "Count: {len(text.split())}\n",
        "\n",
        "      --------------------------------------------------------------------------------\n",
        "{Fore.LIGHTCYAN_EX}\n",
        "Summary:\n",
        "\n",
        "{summary}\n",
        "\n",
        "Count: {len(summary.split())}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeXp4u-b7jzK",
        "outputId": "f6e04c60-948f-4a7b-f65c-21da8216715b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[34m\n",
            "Original:\n",
            "\n",
            "Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words.\n",
            " These vectors capture information about the meaning of the word based on the surrounding words.\n",
            " The word2vec algorithm estimates these representations by modeling text in a large corpus.\n",
            " Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.\n",
            " Word2vec was developed by Tom치코 Mikolov and colleagues at Google and published in 2013.\n",
            "Word2vec represents a word as a high-dimension vector of numbers which capture relationships between words.\n",
            "In particular, words which appear in similar contexts are mapped to vectors which are nearby as measured by cosine similarity. This indicates the level of semantic similarity between the words,\n",
            " so for example the vectors for walk and ran are nearby, as are those for \"but\" and \"however\", and \"Berlin\" and \"Germany\".\n",
            "\n",
            "Count: 141\n",
            "      \n",
            "      --------------------------------------------------------------------------------\n",
            "\u001b[96m\n",
            "Summary:\n",
            "\n",
            "Word2vec represents a word as a high-dimension vector of numbers which capture relationships between words. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. These vectors capture information about the meaning of the word based on the surrounding words.\n",
            "\n",
            "Count: 47\n",
            "\n"
          ]
        }
      ]
    }
  ]
}